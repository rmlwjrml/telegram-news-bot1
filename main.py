import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from telegram import Bot
import pytz
import time
import os

kst = pytz.timezone('Asia/Seoul')
TELEGRAM_TOKEN = "7440645018:AAG_yFBsdyaMmhK_He7lI3EBWggLK9wenXg"
CHAT_ID = "5639589613"
bot = Bot(token=TELEGRAM_TOKEN)

sent_links_file = "sent_links.txt"
sent_links = set()
link_date_map = {}

# âœ… 1. íŒŒì¼ì—ì„œ ë§í¬ ë¶ˆëŸ¬ì˜¤ë©° ì˜¤ë˜ëœ ê²ƒ í•„í„°ë§
def load_sent_links():
    global sent_links, link_date_map
    now = datetime.now(kst)
    three_days_ago = now - timedelta(days=3)

    if os.path.exists(sent_links_file):
        with open(sent_links_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        new_lines = []
        for line in lines:
            parts = line.strip().split("|")
            if len(parts) != 2:
                continue
            link, date_str = parts
            try:
                date_obj = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
                if date_obj >= three_days_ago:
                    sent_links.add(link)
                    link_date_map[link] = date_obj
                    new_lines.append(line.strip())
            except:
                continue
        # ğŸ” ì˜¤ë˜ëœ ê²ƒ ì œê±°ëœ ë‚´ìš©ìœ¼ë¡œ íŒŒì¼ ë®ì–´ì“°ê¸°
        with open(sent_links_file, "w", encoding="utf-8") as f:
            for line in new_lines:
                f.write(line + "\n")

# âœ… 2. ìƒˆ ë§í¬ ì €ì¥
def save_sent_link(link):
    now_str = datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")
    with open(sent_links_file, "a", encoding="utf-8") as f:
        f.write(f"{link}|{now_str}\n")

# âœ… ìµœì´ˆ ë¡œë”© ì‹œ í˜¸ì¶œ
load_sent_links()

# ğŸ” í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ìœ ì§€
keywords = [ "2ì°¨ì „ì§€", "4ì¸ë±…", "4ì¸í„°ë„·", "5G", "AI", "AIê³ ì†ë„ë¡œ", "AIë°˜ë„ì²´", "AR", "AWS", "BMS", "ESS", "FDAìŠ¹ì¸", "GPT", "GPTì¹©", "IRA", "Kë¬¸í™”", "Kë·°í‹°", "Kì½˜í…ì¸ ", "LNG", "NFT", "SG", "SMR", "SOC", "STO", "VR", "ai", "aiê³ ì†ë„ë¡œ", "aië°˜ë„ì²´", "ar", "aws", "bms", "ë¸”ë¡ì²´ì¸", "ì±—GPT", "êµ¬ê¸€", "êµ­ë‚´ìµœì´ˆ", "êµ­ê°€ì „ëµì‚°ì—…", "êµ­ì‚°í™”", "ë°ì´í„°ì„¼í„°", "ëŒ€ë¶ì§€ì›", "ëŒ€ì™•ê³ ë˜", "ëŒ€í†µë ¹", "ëŒ€ì²´ì—ë„ˆì§€", "ë”¥ëŸ¬ë‹", "ë¼ì´ë‹¤", "ë¡œë³´íƒì‹œ", "ë¡œë´‡", "ë¡œë´‡ëˆˆ", "ë¡œë´‡ì†", "ë§ˆê·€ìƒì–´", "ë©”íƒ€", "ë©”íƒ€ë²„ìŠ¤", "ë©´ì—­í•­ì•”ì œ", "ëª¨ë¹Œë¦¬í‹°", "ë°”ì´ì˜¤", "ë°©ì‚¬ëŠ¥", "ë°©ì‚°", "ë°±ì‹ ", "ë¶ê·¹í•­ë¡œ", "ë¸”ë™ì›°", "ë¹„ë§Œì¹˜ë£Œì œ", "ì‚¬ì´ë²„ë³´ì•ˆ", "ì‚°ë¶ˆ", "ìƒìš©í™”", "ìƒëª…ê³µí•™", "ì„ìœ ", "ì„¸ê³„ìµœëŒ€", "ì„¸ê³„ìµœì´ˆ", "ìŠ¤ë§ˆíŠ¸ê³µì¥", "ìŠ¤ë§ˆíŠ¸ì‹œí‹°", "ìŠ¤ë§ˆíŠ¸ì•ˆê²½", "ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬", "ìŠ¤ë§ˆíŠ¸íŒœ", "ì†ë³´", "ìˆ˜ì†Œ", "ìˆ˜ì¶œê³„ì•½", "ìˆ˜ì¶œê³µì‹œ", "ìˆ˜ì¶œí—ˆê°€", "ìˆ˜í•´", "ìŠ¤í…Œì´ë¸”ì½”ì¸", "ìŠ¤íƒ€íŠ¸ì—…", "ì‹œí™©", "ì‹¤ì‹œê°„ ì†ë³´", "ì‹ ì•½", "ì•ˆì „", "ì•Œë˜ìŠ¤ì¹´", "ì•Œì¸ í•˜ì´ë¨¸", "ì• í”Œ", "ì–‘ìì»´í“¨í„°", "ì—ë„ˆì§€ê³ ì†ë„ë¡œ", "ì—ë„ˆì§€ì •ì±…", "ì—ë„ˆì§€ì•ˆë³´", "ì—ë„ˆì§€ì €ì¥ì¥ì¹˜", "ì—”ë¹„ë””ì•„", "ì—¬ë¡ ", "ì˜ˆë¹„íƒ€ë‹¹ì„±", "ì˜µí‹°ë¨¸ìŠ¤", "ì›ìœ ", "ì›ì „", "ì›ì „ê±´ì„¤", "ì›ì „í•´ì²´", "ìœ ì „ìì¹˜ë£Œ", "ì´ì°¨ì „ì§€", "ì´ì¬ëª…", "ì¸í„°ë„·ì€í–‰", "ì„ìƒ", "ì„ìƒ1ìƒ", "ì„ìƒ2ìƒ", "ì„ìƒ3ìƒ", "ì„ìƒì™„ë£Œ", "ììœ¨ì£¼í–‰", "ìë™ì°¨", "ì¬ê±´", "ì¬ë‚œ", "ì¬ìƒì—ë„ˆì§€", "ì „ê¸°ì°¨", "ì „ë ¥", "ì „ë ¥ë§", "ì „ì„ ", "ì „ìê²°ì œ", "ì „ìŸ", "ì „ìŸìœ„í—˜", "ì •ë¹„ì‚¬ì—…", "ì •ë¶€ì£¼ë„", "ì •ë¶€ì§€ì›", "ì œì¬ì™„í™”", "ì œì¬í•´ì œ", "ì§€ì—­í™”í", "ì§€ëŠ¥í˜•ë¡œë´‡", "ì§€ì§„", "ì§„ë‹¨ê¸°ê¸°", "ì§„ë‹¨ì‹œì•½", "ì§„ë‹¨í‚¤íŠ¸", "ì°¨ì„¸ëŒ€ë°°í„°ë¦¬", "ì±—gpt", "ì² ê°•", "ì´ˆê³ ì†ì¸í„°ë„·", "ì´ˆì†Œí˜•ëª¨ë“ˆì›ìë¡œ", "ì´ˆì „ë„ì²´", "ì¹˜ë§¤", "ì¹œí™˜ê²½ì—ë„ˆì§€", "ì¹´ì¹´ì˜¤í˜ì´", "ì¹´ì§€ë…¸", "í´ë¼ìš°ë“œ", "í‚¤ì˜¤ìŠ¤í¬", "íƒ„ì†Œë°°ì¶œê¶Œ", "íƒ„ì†Œì¤‘ë¦½", "íƒ„ì†Œì„¸", "íƒ„ì†Œí¬ì§‘", "í…ŒìŠ¬ë¼", "íŠ¸ëŸ¼í”„", "íŠ¹í—ˆ", "íŠ¹í—ˆë“±ë¡", "íŠ¹í—ˆì·¨ë“", "íŠ¹í—ˆíšë“", "íŠ¹ì§•ì£¼", "íˆ¬ììœ ì¹˜", "í•€í…Œí¬", "í•‘í¬í", "í’ë ¥", "í“¨ë¦¬ì˜¤ì‚¬", "í•´ìƒí’ë ¥", "í•´ì €í„°ë„", "í•´ì™¸ìˆ˜ì£¼", "í•µì‹¬ê¸°ìˆ ", "í•µì‹¬ì†Œì¬", "í•µíê¸°ë¬¼", "í™”ì¥í’ˆ", "í™˜ê²½ê·œì œ", "í™©ì‚¬", "í™©ì‚¬ê²½ë³´", "í™©ì‚¬ì£¼ì˜ë³´", "í™©ì‚¬íŠ¹ë³´", "í™©ì‚¬í”¼í•´", "íœ´ë¨¸ë…¸ì´ë“œ", "íœ´ì „", "í¬í† ë¥˜", "ì­ìŠ¨í™©", "ìš°ì£¼í•­ê³µ", "í•­ê³µì‚¬", "í•­ê³µìš°ì£¼", "í•œë¥˜", "í•œí•œë ¹", "ìƒëª…ê³¼í•™", "ì¤„ê¸°ì„¸í¬", "mRNA", "ms", "ë„¤ì´ë²„í˜ì´", "ë¦¬íŠ¬", "ë‹ˆì¼ˆ", "ë„ì‹œì¬ìƒ", "ê·œì œì™„í™”", "ê°ì„¸", "ì„¸ì œí˜œíƒ", "æ", "å…†", "ç¾", "í•œë¯¸", "í­ì—¼", "ëŸ¬ì‹œì•„", "ìš°í¬ë¼ì´ë‚˜", "ì„¸ì¢…ì´ì „"
]

news_sites = [
    # ìƒëµ ì—†ì´ ê·¸ëŒ€ë¡œ ìœ ì§€
    "https://www.asiae.co.kr/rss/all.xml", "https://rss.etnews.com/ETnews.xml", "https://rss.etnews.com/Section901.xml", "https://rss.etnews.com/Section902.xml", "https://rss.etnews.com/Section903.xml", "https://rss.etnews.com/Section904.xml", "https://rss.etnews.com/02.xml", "https://rss.etnews.com/02024.xml", "https://rss.etnews.com/02027.xml", "https://rss.etnews.com/04.xml", "https://rss.etnews.com/04046.xml", "https://www.hankyung.com/feed", "https://www.edaily.co.kr/rss/news.xml", "https://www.infostockdaily.co.kr/rss/allArticle.xml", "https://www.yonhapnewstv.co.kr/browse/feed/", "https://www.mk.co.kr/rss/30000001/", "https://www.mk.co.kr/rss/40300001/", "https://www.mk.co.kr/rss/30100041/", "https://www.mk.co.kr/rss/30200030/", "https://www.mk.co.kr/rss/30300018/", "https://www.mk.co.kr/rss/50200011/", "https://www.mk.co.kr/rss/50300009/", "https://www.mk.co.kr/rss/30000023/", "https://www.mk.co.kr/rss/71000001/", "https://www.fntimes.com/rss/allArticle.xml", "https://www.seoulfn.com/rss/allArticle.xml", "https://www.kpinews.co.kr/rss/allArticle.xml", "https://www.kbiznews.co.kr/rss/allArticle.xml", "https://www.itooza.com/rss/today.xml", "https://www.kukinews.com/rss/allArticle.xml", "https://www.consumernews.co.kr/rss/allArticle.xml", "https://www.ekn.kr/rss/allArticle.xml", "https://www.paxnet.co.kr/rss/main.xml", "https://biz.chosun.com/rss/chosunbiz.xml", "https://www.sedaily.com/NewsList/GB01", "http://imnews.imbc.com/rss/news/news_00.xml", "http://imnews.imbc.com/rss/news/news_01.xml", "http://imnews.imbc.com/rss/news/news_04.xml", "http://www.chosun.com/site/data/rss/rss.xml", "http://www.khan.co.kr/rss/rssdata/total_news.xml", "http://www.khan.co.kr/rss/rssdata/economy.xml"
]

def fetch_and_filter_news():
    now = datetime.now(kst)
    five_minutes_ago = now - timedelta(minutes=5)

    for url in news_sites:
        try:
            rss = feedparser.parse(url)
            for entry in rss.entries:
                raw_title = entry.title.strip()
                link = entry.link.strip()

                if "youtube.com" in link or "youtu.be" in link:
                    continue

                pub_struct = entry.get("published_parsed") or entry.get("updated_parsed")
                if not pub_struct:
                    continue
                pub_datetime = datetime(*pub_struct[:6], tzinfo=pytz.utc).astimezone(kst)

                if pub_datetime < five_minutes_ago or pub_datetime > now:
                    continue

                if any(domain in url for domain in ["asiae", "edaily", "infostock"]):
                    try:
                        html = requests.get(link, timeout=5)
                        html.encoding = "euc-kr"
                        soup = BeautifulSoup(html.text, "html.parser")
                        og_title = soup.select_one("meta[property='og:title']")
                        if og_title:
                            raw_title = og_title["content"].strip()
                    except:
                        continue

                if link in sent_links:
                    continue
                sent_links.add(link)
                save_sent_link(link)

                if any(k in raw_title for k in keywords):
                    bot.send_message(chat_id=CHAT_ID, text=f"[{raw_title}]\n{link}")

        except Exception:
            continue

if __name__ == "__main__":
    while True:
        fetch_and_filter_news()
        time.sleep(5)
